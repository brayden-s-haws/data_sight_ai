{"cells":[{"cell_type":"code","source":"import pandas as _hex_pandas\nimport datetime as _hex_datetime\nimport json as _hex_json","execution_count":null,"metadata":{},"outputs":[]},{"cell_type":"code","source":"hex_scheduled = _hex_json.loads(\"false\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_user_email = _hex_json.loads(\"\\\"example-user@example.com\\\"\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_run_context = _hex_json.loads(\"\\\"logic\\\"\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_timezone = _hex_json.loads(\"\\\"UTC\\\"\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_project_id = _hex_json.loads(\"\\\"5674db16-661a-4545-916f-ffafe6620ea2\\\"\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_project_name = _hex_json.loads(\"\\\"DataSight AI\\\"\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_status = _hex_json.loads(\"\\\"Exploratory\\\"\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_categories = _hex_json.loads(\"[\\\"External\\\"]\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_color_palette = _hex_json.loads(\"[\\\"#4C78A8\\\",\\\"#F58518\\\",\\\"#E45756\\\",\\\"#72B7B2\\\",\\\"#54A24B\\\",\\\"#EECA3B\\\",\\\"#B279A2\\\",\\\"#FF9DA6\\\",\\\"#9D755D\\\",\\\"#BAB0AC\\\"]\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"markdown","source":"This app allows users to quickly onboard themselves to the contents of a database. It uses standard data tools and Generative AI agents to surface information about the data. Currently it offers: \n* AI generated descriptions for each table\n* Data preview and column stats \n* AI generated table relationship details \n* Natural language querying via an LLM-based agent (this agent is built with error handling and can recover from errors to generate a valid query)\n\n(This is meant to be a proof of concept for how AI tools can help users better understand and work with their data.)\n\nComing Soon üó∫Ô∏è:\n* AI generated ERDs\n* AI recommended visualizations\n\nYou can get technical details and access to the code on [GitHub](https://github.com/brayden-s-haws/data_sight_ai).","metadata":{}},{"cell_type":"code","source":"\"\"\"\nInstall non-standard packages\n\"\"\"\n\n!pip install langchain langchain-experimental\n!pip install sqlalchemy psycopg2\n!pip install psycopg2-binary\n!pip install --upgrade --force-reinstall psycopg2-binary\n\n","metadata":{},"execution_count":null,"outputs":[{"data":{"text/plain":"Requirement already satisfied: langchain in /home/hexuser/.cache/pypoetry/virtualenvs/python-kernel-OtKFaj5M-py3.9/lib/python3.9/site-packages (0.0.347)\nCollecting langchain-experimental\n  Downloading langchain_experimental-0.0.49-py3-none-any.whl.metadata (1.9 kB)\nRequirement already satisfied: PyYAML>=5.3 in /home/hexuser/.cache/pypoetry/virtualenvs/python-kernel-OtKFaj5M-py3.9/lib/python3.9/site-packages (from langchain) (6.0.1)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /home/hexuser/.cache/pypoetry/virtualenvs/python-kernel-OtKFaj5M-py3.9/lib/python3.9/site-packages (from langchain) (1.4.25)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/hexuser/.cache/pypoetry/virtualenvs/python-kernel-OtKFaj5M-py3.9/lib/python3.9/site-packages (from langchain) (3.8.5)\nRequirement already satisfied: anyio<4.0 in /home/hexuser/.cache/pypoetry/virtualenvs/python-kernel-OtKFaj5M-py3.9/lib/python3.9/site-packages (from langchain) (3.6.2)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /home/hexuser/.cache/pypoetry/virtualenvs/python-kernel-OtKFaj5M-py3.9/lib/python3.9/site-packages (from langchain) (4.0.2)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/hexuser/.cache/pypoetry/virtualenvs/python-kernel-OtKFaj5M-py3.9/lib/python3.9/site-packages (from langchain) (0.5.9)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /home/hexuser/.cache/pypoetry/virtualenvs/python-kernel-OtKFaj5M-py3.9/lib/python3.9/site-packages (from langchain) (1.33)\nRequirement already satisfied: langchain-core<0.1,>=0.0.11 in /home/hexuser/.cache/pypoetry/virtualenvs/python-kernel-OtKFaj5M-py3.9/lib/python3.9/site-packages (from langchain) (0.0.11)\nRequirement already satisfied: langsmith<0.1.0,>=0.0.63 in /home/hexuser/.cache/pypoetry/virtualenvs/python-kernel-OtKFaj5M-py3.9/lib/python3.9/site-packages (from langchain) (0.0.69)\nRequirement already satisfied: numpy<2,>=1 in /home/hexuser/.cache/pypoetry/virtualenvs/python-kernel-OtKFaj5M-py3.9/lib/python3.9/site-packages (from langchain) (1.23.4)\nRequirement already satisfied: pydantic<3,>=1 in /home/hexuser/.cache/pypoetry/virtualenvs/python-kernel-OtKFaj5M-py3.9/lib/python3.9/site-packages (from langchain) (1.10.12)\nRequirement already satisfied: requests<3,>=2 in /home/hexuser/.cache/pypoetry/virtualenvs/python-kernel-OtKFaj5M-py3.9/lib/python3.9/site-packages (from langchain) (2.28.0)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/hexuser/.cache/pypoetry/virtualenvs/python-kernel-OtKFaj5M-py3.9/lib/python3.9/site-packages (from langchain) (8.2.2)\nCollecting langchain\n  Downloading langchain-0.1.1-py3-none-any.whl.metadata (13 kB)\nINFO: pip is looking at multiple versions of langchain-experimental to determine which version is compatible with other requirements. This could take a while.\nCollecting langchain-experimental\n  Downloading langchain_experimental-0.0.48-py3-none-any.whl.metadata (1.9 kB)\n  Downloading langchain_experimental-0.0.47-py3-none-any.whl.metadata (1.9 kB)\nCollecting langchain\n  Downloading langchain-0.0.354-py3-none-any.whl.metadata (13 kB)\nCollecting langchain-experimental\n  Downloading langchain_experimental-0.0.46-py3-none-any.whl.metadata (1.9 kB)\nCollecting langchain-core<0.1,>=0.0.11 (from langchain)\n  Downloading langchain_core-0.0.13-py3-none-any.whl.metadata (978 bytes)\nCollecting langchain-community<0.1,>=0.0.8 (from langchain)\n  Downloading langchain_community-0.0.13-py3-none-any.whl.metadata (7.5 kB)\nINFO: pip is looking at multiple versions of langchain to determine which version is compatible with other requirements. This could take a while.\nCollecting langchain\n  Downloading langchain-0.0.353-py3-none-any.whl.metadata (13 kB)\n  Downloading langchain-0.0.352-py3-none-any.whl.metadata (13 kB)\n  Downloading langchain-0.0.351-py3-none-any.whl.metadata (13 kB)\n  Downloading langchain-0.0.350-py3-none-any.whl.metadata (13 kB)\n  Downloading langchain-0.0.349-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: attrs>=17.3.0 in /home/hexuser/.cache/pypoetry/virtualenvs/python-kernel-OtKFaj5M-py3.9/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/hexuser/.cache/pypoetry/virtualenvs/python-kernel-OtKFaj5M-py3.9/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.6)\nRequirement already satisfied: multidict<7.0,>=4.5 in /home/hexuser/.cache/pypoetry/virtualenvs/python-kernel-OtKFaj5M-py3.9/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /home/hexuser/.cache/pypoetry/virtualenvs/python-kernel-OtKFaj5M-py3.9/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.7.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /home/hexuser/.cache/pypoetry/virtualenvs/python-kernel-OtKFaj5M-py3.9/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /home/hexuser/.cache/pypoetry/virtualenvs/python-kernel-OtKFaj5M-py3.9/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\nRequirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /home/hexuser/.cache/pypoetry/virtualenvs/python-kernel-OtKFaj5M-py3.9/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.17.0)\nRequirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /home/hexuser/.cache/pypoetry/virtualenvs/python-kernel-OtKFaj5M-py3.9/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (1.5.1)\nRequirement already satisfied: typing-inspect>=0.4.0 in /home/hexuser/.cache/pypoetry/virtualenvs/python-kernel-OtKFaj5M-py3.9/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\nRequirement already satisfied: jsonpointer>=1.9 in /home/hexuser/.cache/pypoetry/virtualenvs/python-kernel-OtKFaj5M-py3.9/lib/python3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\nINFO: pip is looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\nCollecting langchain-community<0.1,>=0.0.1 (from langchain)\n  Downloading langchain_community-0.0.12-py3-none-any.whl.metadata (7.5 kB)\n  Downloading langchain_community-0.0.11-py3-none-any.whl.metadata (7.3 kB)\n  Downloading langchain_community-0.0.10-py3-none-any.whl.metadata (7.3 kB)\n  Downloading langchain_community-0.0.8-py3-none-any.whl.metadata (7.3 kB)\n  Downloading langchain_community-0.0.7-py3-none-any.whl.metadata (7.3 kB)\n  Downloading langchain_community-0.0.6-py3-none-any.whl.metadata (7.2 kB)\n  Downloading langchain_community-0.0.5-py3-none-any.whl.metadata (7.1 kB)\nINFO: pip is still looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\n  Downloading langchain_community-0.0.4-py3-none-any.whl.metadata (7.0 kB)\n  Downloading langchain_community-0.0.3-py3-none-any.whl.metadata (7.0 kB)\n  Downloading langchain_community-0.0.2-py3-none-any.whl.metadata (5.8 kB)\n  Downloading langchain_community-0.0.1-py3-none-any.whl.metadata (5.8 kB)\nCollecting packaging<24.0,>=23.2 (from langchain-core<0.1,>=0.0.11->langchain)\n  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: typing-extensions>=4.2.0 in /home/hexuser/.cache/pypoetry/virtualenvs/python-kernel-OtKFaj5M-py3.9/lib/python3.9/site-packages (from pydantic<3,>=1->langchain) (4.7.1)\nRequirement already satisfied: idna<4,>=2.5 in /home/hexuser/.cache/pypoetry/virtualenvs/python-kernel-OtKFaj5M-py3.9/lib/python3.9/site-packages (from requests<3,>=2->langchain) (3.2)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /home/hexuser/.cache/pypoetry/virtualenvs/python-kernel-OtKFaj5M-py3.9/lib/python3.9/site-packages (from requests<3,>=2->langchain) (1.26.16)\nRequirement already satisfied: certifi>=2017.4.17 in /home/hexuser/.cache/pypoetry/virtualenvs/python-kernel-OtKFaj5M-py3.9/lib/python3.9/site-packages (from requests<3,>=2->langchain) (2021.10.8)\nRequirement already satisfied: greenlet!=0.4.17 in /home/hexuser/.cache/pypoetry/virtualenvs/python-kernel-OtKFaj5M-py3.9/lib/python3.9/site-packages (from SQLAlchemy<3,>=1.4->langchain) (1.1.2)\nRequirement already satisfied: sniffio>=1.1 in /home/hexuser/.cache/pypoetry/virtualenvs/python-kernel-OtKFaj5M-py3.9/lib/python3.9/site-packages (from anyio<4.0->langchain) (1.3.0)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /home/hexuser/.cache/pypoetry/virtualenvs/python-kernel-OtKFaj5M-py3.9/lib/python3.9/site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (0.4.3)\nDownloading langchain_experimental-0.0.46-py3-none-any.whl (162 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m163.0/163.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain-0.0.349-py3-none-any.whl (808 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m808.6/808.6 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading langchain_community-0.0.1-py3-none-any.whl (1.5 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_core-0.0.13-py3-none-any.whl (188 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m188.2/188.2 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading packaging-23.2-py3-none-any.whl (53 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: packaging, langchain-core, langchain-community, langchain, langchain-experimental\n  Attempting uninstall: packaging\n    Found existing installation: packaging 21.3\n    Uninstalling packaging-21.3:\n      Successfully uninstalled packaging-21.3\n  Attempting uninstall: langchain-core\n    Found existing installation: langchain-core 0.0.11\n    Uninstalling langchain-core-0.0.11:\n      Successfully uninstalled langchain-core-0.0.11\n  Attempting uninstall: langchain\n    Found existing installation: langchain 0.0.347\n    Uninstalling langchain-0.0.347:\n      Successfully uninstalled langchain-0.0.347\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nhex-packages 0.1.0 requires packaging==21.3, but you have packaging 23.2 which is incompatible.\ntableauserverclient 0.24 requires packaging~=21.3, but you have packaging 23.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed langchain-0.0.349 langchain-community-0.0.1 langchain-core-0.0.13 langchain-experimental-0.0.46 packaging-23.2\nRequirement already satisfied: sqlalchemy in /home/hexuser/.cache/pypoetry/virtualenvs/python-kernel-OtKFaj5M-py3.9/lib/python3.9/site-packages (1.4.25)\nRequirement already satisfied: psycopg2 in /home/hexuser/.cache/pypoetry/virtualenvs/python-kernel-OtKFaj5M-py3.9/lib/python3.9/site-packages (2.9.1)\nRequirement already satisfied: greenlet!=0.4.17 in /home/hexuser/.cache/pypoetry/virtualenvs/python-kernel-OtKFaj5M-py3.9/lib/python3.9/site-packages (from sqlalchemy) (1.1.2)\nCollecting psycopg2-binary\n  Downloading psycopg2_binary-2.9.9-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\nDownloading psycopg2_binary-2.9.9-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: psycopg2-binary\nSuccessfully installed psycopg2-binary-2.9.9\nCollecting psycopg2-binary\n  Using cached psycopg2_binary-2.9.9-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\nUsing cached psycopg2_binary-2.9.9-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\nInstalling collected packages: psycopg2-binary\n  Attempting uninstall: psycopg2-binary\n    Found existing installation: psycopg2-binary 2.9.9\n    Uninstalling psycopg2-binary-2.9.9:\n      Successfully uninstalled psycopg2-binary-2.9.9\nSuccessfully installed psycopg2-binary-2.9.9\n"},"execution_count":null,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","source":"select_database = _hex_json.loads(\"\\\"lego\\\"\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sqlalchemy import create_engine\n\n# PostgreSQL connection parameters\nusername = postgres_username\npassword = postgres_password\nhost = postgres_host\nport = '5432'\ndatabase = select_database\nendpoint_id = postgres_endpoint_id\n\n# PostgreSQL connection string\npostgresql_connection_string = f'postgresql://{username}:{password}@{host}:{port}/{database}?sslmode=require&options=endpoint%3D{endpoint_id}'\n\n# Create SQLAlchemy engine for PostgreSQL\nengine = create_engine(postgresql_connection_string)\n\n# Test the connection\ntry:\n    with engine.connect() as connection:\n        print(\"Successfully connected to PostgreSQL\")\nexcept Exception as e:\n    print(f\"Error connecting to PostgreSQL: {e}\")\n","metadata":{},"execution_count":null,"outputs":[{"data":{"text/plain":"Successfully connected to PostgreSQL\n"},"execution_count":null,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","source":"table_name_query = \"\"\"\nSELECT table_name\nFROM information_schema.tables\nWHERE table_schema = 'public'\n\"\"\"\n\nwith engine.connect() as connection:\n    result = connection.execute(table_name_query)\n    tables = [row['table_name'] for row in result]\n\n# Printing the table names\nfor table in tables:\n    print(table)\n\n\ndatabase_table_names = []\nfor table in tables:\n    database_table_names.append(table)","metadata":{},"execution_count":null,"outputs":[{"data":{"text/plain":"lego_colors\nlego_inventories\nlego_inventory_parts\nlego_inventory_sets\nlego_part_categories\nlego_parts\nlego_sets\nlego_themes\n"},"execution_count":null,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","source":"import json as _hex_json\nfocus_table = _hex_pks.kernel_execution.input_cell.run_dropdown_dynamic(args=_hex_types.DropdownDynamicArgs.from_dict({**_hex_json.loads(\"{\\\"dataframe_column\\\":null,\\\"ui_selected_value\\\":\\\"lego_parts\\\"}\"), **{_hex_json.loads(\"\\\"options_variable\\\"\"):_hex_kernel.variable_or_none(\"database_table_names\", scope_getter=lambda: globals())}}), app_session_token=_hex_APP_SESSION_TOKEN, python_kernel_init_status=_hex_python_kernel_init_status, hex_timezone=_hex_kernel.variable_or_none(\"hex_timezone\", scope_getter=lambda: globals()), interrupt_event=locals().get(\"_hex_interrupt_event\"))\n\nimport json as _hex_json\n_hex_pks.kernel_execution.input_cell.filled_dynamic_value(args=_hex_types.FilledDynamicValueArgs.from_dict({**_hex_json.loads(\"{\\\"variable_name\\\":\\\"database_table_names\\\",\\\"dataframe_column\\\":null,\\\"max_size\\\":10000,\\\"max_size_in_bytes\\\":5242880}\"), **{_hex_json.loads(\"\\\"variable\\\"\"):_hex_kernel.variable_or_none(\"database_table_names\", scope_getter=lambda: globals())}}), app_session_token=_hex_APP_SESSION_TOKEN, python_kernel_init_status=_hex_python_kernel_init_status, hex_timezone=_hex_kernel.variable_or_none(\"hex_timezone\", scope_getter=lambda: globals()), interrupt_event=locals().get(\"_hex_interrupt_event\"))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rows_to_preview = _hex_json.loads(\"15\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nGenerate a preview of the selected table and sample size\n\"\"\"\n\nimport pandas as pd\n\npreview_query = f\"\"\"\nSELECT * \nFROM {focus_table}\nLIMIT {rows_to_preview}\n\"\"\"\n\nstats_query = f\"\"\"\nSELECT *\nFROM {focus_table}\n\"\"\"\n\nwith engine.connect() as connection:\n    table_preview_dataframe = pd.read_sql_query(preview_query, connection)\n    table_stats_dataframe = pd.read_sql_query(stats_query, connection)\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"table_preview_dataframe","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nDisplay stats for all columns in the table\n\"\"\"\n\ntable_stats_dataframe.describe(include='all', datetime_is_numeric=True)\n","metadata":{},"execution_count":null,"outputs":[{"data":{"application/vnd.hex.export+parquet":{"success":true,"exportKey":"455658aa-ee04-480f-945a-3fd455933fa2/5674db16-661a-4545-916f-ffafe6620ea2/exports/4d4e4aab-b3c7-4a60-9147-090862f7a728"},"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>part_num</th>\n      <th>name</th>\n      <th>part_cat_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>25993</td>\n      <td>25993</td>\n      <td>25993.000000</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>25993</td>\n      <td>25779</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>0687b1</td>\n      <td>Minifig</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>1</td>\n      <td>38</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>19.491825</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>12.314862</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>27.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>57.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"execution_count":null,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","source":"\"\"\"\nUse Langchain to generate and store a table description. First look up if a description exists, if so display it. If one does not exist then generate a description.\n\"\"\"\n\nimport csv\nimport os\nfrom langchain.agents import create_sql_agent\nfrom langchain.agents.agent_toolkits import SQLDatabaseToolkit\nfrom langchain.utilities import SQLDatabase\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.agents.agent_types import AgentType\n\ncsv_filename = \"table_descriptions.csv\"\ndescribe_prompt = f\"Describe {focus_table}. Give a one sentence description of the table. And give a sentence on how the data in this table might be used by a user. Do not repeat information between sentences\"\n\ndb = SQLDatabase(engine, schema='public')\nllm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0,  max_tokens=2000)\n\nagent_executor = create_sql_agent(\n    llm=llm,\n    toolkit=SQLDatabaseToolkit(db=db, llm=llm),\n    verbose=True, use_query_checker=True, handle_parsing_errors=True,\n    agent_type=AgentType.OPENAI_FUNCTIONS, agent_executor_kwargs={\"handle_parsing_errors\": True},\n)\n\ndb_tables = SQLDatabase(engine)\n\nagent_executor_tables = create_sql_agent(\n    llm=llm,\n    toolkit=SQLDatabaseToolkit(db=db_tables, llm=llm),\n    verbose=True, use_query_checker=True, handle_parsing_errors=True, top_k=10,\n    agent_type=AgentType.OPENAI_FUNCTIONS, agent_executor_kwargs={\"handle_parsing_errors\": True},\n)\n\n# Check if the CSV file exists and read it into a dictionary\nif os.path.exists(csv_filename):\n    with open(csv_filename, mode=\"r\", newline=\"\") as csvfile:\n        reader = csv.DictReader(csvfile)\n        description_dict = {row[\"key\"]: row[\"description\"] for row in reader}\nelse:\n    description_dict = {}\n\n# Check if the description for table_to_query exists\nif select_database+'.'+focus_table in description_dict:\n    print(description_dict[select_database+'.'+focus_table])\nelse:\n    # Run the db_chain and store the output\n    description_output = agent_executor.run(describe_prompt)\n    print(description_output)  \n\n    # Add the new record to the CSV file\n    with open(csv_filename, mode=\"a\", newline=\"\") as csvfile:\n        fieldnames = [\"key\", \"description\"]\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        if not description_dict:  # If the dictionary is empty, write the header\n            writer.writeheader()\n        writer.writerow({\"key\": select_database+'.'+focus_table, \"description\": description_output})","metadata":{},"execution_count":null,"outputs":[{"data":{"text/plain":"\n\n\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n\u001b[32;1m\u001b[1;3m\nInvoking: `sql_db_list_tables` with `{}`\n\n\n\u001b[0m\u001b[38;5;200m\u001b[1;3mlego_colors, lego_inventories, lego_inventory_parts, lego_inventory_sets, lego_part_categories, lego_parts, lego_sets, lego_themes\u001b[0m\u001b[32;1m\u001b[1;3m\nInvoking: `sql_db_schema` with `lego_parts`\n\n\n\u001b[0m\u001b[33;1m\u001b[1;3m\nCREATE TABLE public.lego_parts (\n\tpart_num VARCHAR(255) NOT NULL, \n\tname TEXT NOT NULL, \n\tpart_cat_id INTEGER NOT NULL, \n\tCONSTRAINT lego_parts_pkey PRIMARY KEY (part_num)\n)\n\n/*\n3 rows from lego_parts table:\npart_num\tname\tpart_cat_id\n0687b1\tSet 0687 Activity Booklet 1\t17\n0901\tBaseplate 16 x 30 with Set 080 Yellow House Print\t1\n0902\tBaseplate 16 x 24 with Set 080 Small White House Print\t1\n*/\u001b[0m\u001b[32;1m\u001b[1;3mThe `lego_parts` table contains information about LEGO parts, including the part number, name, and part category ID.\n\nThe data in this table can be used by a user to search for specific LEGO parts by their part number or name, and to determine the category of a part.\u001b[0m\n\n\u001b[1m> Finished chain.\u001b[0m\nThe `lego_parts` table contains information about LEGO parts, including the part number, name, and part category ID.\n\nThe data in this table can be used by a user to search for specific LEGO parts by their part number or name, and to determine the category of a part.\n"},"execution_count":null,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","source":"\"\"\"\nUse Langchain to generate an explanation of relationships between tables\n\"\"\"\n\n# TODO: Figure out why include tables sometimes does not work\n\ncsv_filename = \"table_relationships.csv\"\nrelationship_prompt = f' Describe the relationship between {focus_table} and the other tables in the dataset. If there is a join then describe the join with this pattern \"{focus_table}.column=table.column\"'\nrelationship_prompt_tables = f' Describe the relationship between {focus_table} and the other tables in the dataset: {database_table_names}. If there is a join then describe the join with this pattern \"{focus_table}.column=table.column\"'\n\nllm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0, max_tokens=2000)\n\ntry:\n    # Initialize database with include_tables\n    db = SQLDatabase(engine, schema='public', include_tables=database_table_names)\n    agent_executor = create_sql_agent(\n        llm=llm,\n        toolkit=SQLDatabaseToolkit(db=db, llm=llm),\n        verbose=True, use_query_checker=True, handle_parsing_errors=True,\n        agent_type=AgentType.OPENAI_FUNCTIONS, agent_executor_kwargs={\"handle_parsing_errors\": True},\n    )\nexcept ValueError:\n    # Fallback to initialization without include_tables\n    db = SQLDatabase(engine)\n    agent_executor = create_sql_agent(\n        llm=llm,\n        toolkit=SQLDatabaseToolkit(db=db, llm=llm),\n        verbose=True, use_query_checker=True, handle_parsing_errors=True, top_k=10,\n        agent_type=AgentType.OPENAI_FUNCTIONS, agent_executor_kwargs={\"handle_parsing_errors\": True},\n    )\n\n# Check if the CSV file exists and read it into a dictionary\nif os.path.exists(csv_filename):\n    with open(csv_filename, mode=\"r\", newline=\"\") as csvfile:\n        reader = csv.DictReader(csvfile)\n        relationship_dict = {row[\"key\"]: row[\"relationship\"] for row in reader}\nelse:\n    relationship_dict = {}\n\n# Check if the relationship details for table_to_query exists\nif select_database+'.'+focus_table in relationship_dict:\n    print(relationship_dict[select_database+'.'+focus_table])\nelse:\n    try:\n        # Run the db_chain and store the output\n        relationship_output = agent_executor.run(relationship_prompt)\n    except Exception as error:\n        print(f\"An error occurred: {error}\")\n        # Optionally, handle the error or provide an alternative action\n\n    print(relationship_output)\n\n    # Add the new record to the CSV file\n    with open(csv_filename, mode=\"a\", newline=\"\") as csvfile:\n        fieldnames = [\"key\", \"relationship\"]\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        if not relationship_dict:  # If the dictionary is empty, write the header\n            writer.writeheader()\n        writer.writerow({\"key\": select_database+'.'+focus_table, \"relationship\": relationship_output})\n","metadata":{},"execution_count":null,"outputs":[{"data":{"text/plain":"\n\n\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n\u001b[32;1m\u001b[1;3m\nInvoking: `sql_db_list_tables` with `{}`\n\n\n\u001b[0m\u001b[38;5;200m\u001b[1;3mlego_colors, lego_inventories, lego_inventory_parts, lego_inventory_sets, lego_part_categories, lego_parts, lego_sets, lego_themes\u001b[0m\u001b[32;1m\u001b[1;3m\nInvoking: `sql_db_schema` with `lego_parts`\n\n\n\u001b[0m\u001b[33;1m\u001b[1;3m\nCREATE TABLE public.lego_parts (\n\tpart_num VARCHAR(255) NOT NULL, \n\tname TEXT NOT NULL, \n\tpart_cat_id INTEGER NOT NULL, \n\tCONSTRAINT lego_parts_pkey PRIMARY KEY (part_num)\n)\n\n/*\n3 rows from lego_parts table:\npart_num\tname\tpart_cat_id\n0687b1\tSet 0687 Activity Booklet 1\t17\n0901\tBaseplate 16 x 30 with Set 080 Yellow House Print\t1\n0902\tBaseplate 16 x 24 with Set 080 Small White House Print\t1\n*/\u001b[0m\u001b[32;1m\u001b[1;3mThe table `lego_parts` has a primary key `part_num` and the following columns:\n\n- `part_num`: VARCHAR(255)\n- `name`: TEXT\n- `part_cat_id`: INTEGER\n\nThere is a relationship between the `lego_parts` table and the `lego_part_categories` table. The relationship is established through the `part_cat_id` column in the `lego_parts` table, which references the primary key of the `lego_part_categories` table.\n\nTo describe the join between `lego_parts` and `lego_part_categories`, the pattern would be `lego_parts.part_cat_id = lego_part_categories.id`.\u001b[0m\n\n\u001b[1m> Finished chain.\u001b[0m\nThe table `lego_parts` has a primary key `part_num` and the following columns:\n\n- `part_num`: VARCHAR(255)\n- `name`: TEXT\n- `part_cat_id`: INTEGER\n\nThere is a relationship between the `lego_parts` table and the `lego_part_categories` table. The relationship is established through the `part_cat_id` column in the `lego_parts` table, which references the primary key of the `lego_part_categories` table.\n\nTo describe the join between `lego_parts` and `lego_part_categories`, the pattern would be `lego_parts.part_cat_id = lego_part_categories.id`.\n"},"execution_count":null,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","source":"user_prompt = _hex_json.loads(\"\\\"Who are the most popular artists\\\"\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"run_prompt = _hex_json.loads(\"false\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This shows the AI agents entire chain of thought and outputs a final query and natural language answer. It can be modified to only display the final result.","metadata":{}},{"cell_type":"code","source":"\"\"\"\nUse Langchain to query the database for the user. Converts their natural language prompt to SQL.\n\"\"\"\n# TODO: Figure out why include_tables isnt working with some data sources. Seems like this needs to be passed into the agent as a custom tool or something?\n# TODO: Add custom prompt (tell it to include the final SQL query in the answer and raw results).\n\nsystem_prompt = f' in {focus_table}.You are a postgres expert. You are able quickly review the tables in a database and understand the contents of each table along with their relation. You will be asked a question for which you need to generate and execute a query. The table in the question is the main focus of the question, but you may also need to join to other tables, so keep them in mind as your create your plan. The column names may not match 1:1 in the prompt, use your best reasoning to select a column (for instance a user may ask for an account but in the table the column is account_name).Ensure that the columns you use in the query exist in the table. As you answer the users question, consider what other columns may be additive to their question and include those in your response. Additionally, include the final SQL query in your answer. Ensure that you run the final SQL query and use it to provide a final, answer to the user that answers their question in natural language. Use a format like this: \"final query\" \\n \"raw results \\n \"natural langauge answer\"'\nfull_prompt = user_prompt + system_prompt\ntables_included_prompt = f' in {focus_table}.You are a postgres expert. You are able quickly review the tables in a database and understand the contents of each table along with their relation. You will be asked a question for which you need to generate and execute a query. The table in the question is the main focus of the question, but you may also need to join to other tables, so keep them in mind as your create your plan, these other tables are {database_table_names}. The column names may not match 1:1 in the prompt, use your best reasoning to select a column (for instance a user may ask for an account but in the table the column is account_name).Ensure that the columns you use in the query exist in the table. As you answer the users question, consider what other columns may be additive to their question and include those in your response. Additionally, include the final SQL query in your answer. Ensure that you run the final SQL query and use it to provide a final, answer to the user that answers their question in natural language. Use a format like this: \"final query\" \\n \"raw results \\n \"natural langauge answer\"'\n\ndisplay(\"Note: This shows the AI agents entire chain of thought and outputs a final query and natural language answer. It can be modified to only display the final result.\")\n\nif run_prompt:\n    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0,  max_tokens=2000, verbose=True)\n\n    try:\n        db = SQLDatabase(engine, include_tables=database_table_names)\n        agent_executor = create_sql_agent(\n            llm=llm,\n            toolkit=SQLDatabaseToolkit(db=db, llm=llm),\n            verbose=True, use_query_checker=True, handle_parsing_errors=True, top_k=10,\n            agent_type=AgentType.OPENAI_FUNCTIONS, agent_executor_kwargs={\"handle_parsing_errors\": True},\n        )\n        agent_executor.run(full_prompt)\n    except ValueError:\n        # Handle the specific error related to include_tables\n        db_tables = SQLDatabase(engine)\n        agent_executor_tables = create_sql_agent(\n            llm=llm,\n            toolkit=SQLDatabaseToolkit(db=db_tables, llm=llm),\n            verbose=True, use_query_checker=True, handle_parsing_errors=True, top_k=10,\n            agent_type=AgentType.OPENAI_FUNCTIONS, agent_executor_kwargs={\"handle_parsing_errors\": True},\n        )\n        agent_executor_tables.run(tables_included_prompt)\n    except Exception as other_error:\n        # Handle other types of exceptions\n        print(f\"An unexpected error occurred: {other_error}\")\n\nelse: \n    display(\"Waiting on you to run the query\")","metadata":{},"execution_count":null,"outputs":[{"data":{"text/plain":"'Note: This shows the AI agents entire chain of thought and outputs a final query and natural language answer. It can be modified to only display the final result.'"},"execution_count":null,"metadata":{},"output_type":"execute_result"},{"data":{"text/plain":"'Waiting on you to run the query'"},"execution_count":null,"metadata":{},"output_type":"execute_result"}]}],"metadata":{"orig_nbformat":4,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"hex_info":{"author":null,"project_id":"5674db16-661a-4545-916f-ffafe6620ea2","version":"draft","exported_date":"Wed Jan 17 2024 20:40:43 GMT+0000 (Coordinated Universal Time)"}},"nbformat":4,"nbformat_minor":4}